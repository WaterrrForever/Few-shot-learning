### 小样本问题的引出

- 广泛认为，在ImageNet（或者更大的数据集）上训练一个backbone，然后再微调是最好的方式。因此，==few—shot learning需要 finetune backone==。我们知道ImageNet图片丰富含量广泛，可以近似看作对真实世界数据分布的刻画，因此希望在ImageNet上训练的模型能够提取通用的图片特征，而这种通用的特征很可能能迁移到下游一个没有见过的图片域。

- transfer learning有一个区别于domain adaptation的非常关键的点，即训练时的数据集和微调时的数据集的图片不仅domain不同，category也通常是不一样的。
|domain（领域）不同|category（类别）不同|
|-|-|
|backbone提取的特征也不够discriminative了，因此需要finetune backbone|导致微调时原有的网络分类层不能用了，得重新学一个|


- 但是在transfer的setting下，是假设我们能够接触到足够多的目标数据集的labeled data的，但在实际应用时，往往==目标数据集的labeled data是不足的==。例如在目标域label data极少的工业零件或者罕见疾病下，从ImageNet finetune的效果也是极差。由此，引出了极具价值的few-shot image classification。

- 人们开始如何提高这种setting下的准确度。人们往往基于改进benchmark（基准），例如miniImageNet满足了category gap，但依然存在较大的domain gap。后续提出==cross-domain few-shot learning==的benchmark 以及==Meta-Dataset==。这些研究使得few-shot learning与实际应用场景的gap迅速缩小。

### 小样本的两个本质问题
#### 1.训练学得一个good representation
- 我们现在清楚了，few-shot image classification其实等价于限制目标域labeled data数量的transfer learning，那么问题来了，transfer learning基本就finetune一条路，玩不出花，为啥一旦把目标域数据量限制到很小，就出现了各种百花齐放的方法呢？这些方法包括但不仅限于meta-learning、conventional training、metric-based method、generation-based method、linear classification、dense-feature based method。

- 这源于finetune的效果和labeled data数据总量成正相关，当finetune调整参数过多很容易造成过拟合。这也解释了为什么MAML这类基于finetune的方法在few-shot learning下表现明显不如metric-based method等其他冻住backbone的方法。

- 基于此就引出了few-shot learning的一个关键的问题：==如何使得从backbone引导出的feature space足够general，足够generalizable==。好的，few-shot learning本质问题至此来到了vision problem的深水区：现有学得的visual representation在很奇怪的图片上时仍然存在怎样的问题？这是区别于传统的传统transfer learning的关键问题。从早期的元学习，到后来metric-based pretraining（cosine classifier）以及加各种自监督学习、蒸馏学习的loss，目标都是学一个更好的特征表示。
 
- 至此，又有人引出一个问题，是不是源数据量足够大，特征表示就足够好，小样本分类问题就解决了？答案应该是，partially solved。首先小样本分类效果和源域数据集大小在绝大部分目标数据集上是正相关关系，因此增大训练数据量是一个非常好的途径；但是，实验发现，这一增长在某些domain gap差距较大的数据集上，**特别是**==实际遇到的真实应用场景中，是有上限的。如果不能从根本探究清楚pretrained visual representation在小样本下存在的问题，或者不使用除finetune之外的目标数据集adaptation方法，这一瓶颈看上去将无法解决。因此，few-shot image classfication这一问题有其独特价值==。

#### 2.测试时从有限labeled data建立一个好的分类器

- 训练从源域学得general image representation之后，在测试时，目标域few-shot任务的所有图片，不管是support（训练）图片还是query（测试）图片，大部分方法均会先将其转为representation再进行下一步操作。这导向另一个问题，即在给定的representation下，如何最大化利用support set少量图片的representation构造一个分类器，使该分类器具有良好泛化能力？
- 把图像represention的潜力发挥到极致的方法很多，而这直接导致了few-shot learning方法的百花齐放。
-  （1）元学习方法，从训练开始就target这一问题，但这些元学习方法忽略了一个重要问题：训练源数据分布和测试时的目标数据分布是不同的，而这直接导致元学习的任务同分布假设不成立，这是元学习效果不佳的重要原因之一。
- （2）由于1. 目标域labeled data少   2.目标域类别在训练时没见过。因此backbone网络会不知道在纷繁复杂的图片应该关注什么信息。（比如一张图，一个人牵着一只狗，标签为人，但由于网络在训练时可能只把狗作为标签（比如imagenet），因此提取特征时便关注狗去了，而不是人。）为解决这类问题，dense-feature based方法应运而生，其核心思想是backbone出来的feature不过global pooling，保留spatial信息，对比不同图片的spatial feature map，从中找出对应关系，这样如果有两张图，其共性是人而不是狗，那通过这种人和人的对应关系就能把狗这一confounding factor给去除。这一类方法论文如：CAN、CTX、DeepEMD 、LDAMF、MCL。

### 小样本未来方向

- 可以看到，**训练学得一个good representation**，和**测试时从有限labeled data建立一个好的分类器**在一般的任务中是可以统一起来的。但在few-shot learning中，随着元学习方法的缺点不断被挖掘，这两点割裂开来，成为两个独立的问题。前者涉及vision representation的本质问题，若为了涨效果可以照搬cv近期各自提升feature质量的trick，比如对比学习、蒸馏等等，成为了各大cv顶会刷点必备，这些方法水一水是可以的，但要真正解决问题，还是要探究visual representation在目标域labeled data是few-shot时所存在的核心问题，这样的研究最近是有，但很少；后者涉及如何给定pretrained feature，做到快速task adaptation，核心点是 1. 取pretrained feature之精华，去其糟粕 2. 从**support set feature及目标query feature中最大化可用信息**，比如从support set中找类内共性，或者找support feature和query feature之间的对应关系，或者从训练集中找寻并利用和support set的相似图片,这第二点可以统称为task adaptation。

- 最后meta-dataset，这个benchmark非常接近真实场景，其中multi-domain FSL的setting从根本上解决了训练集单一domain泛化差的问题，根除了元学习方法的泛化障碍，可能能够使得task adaptation方法更加自然、有效，是一种可能的真正解决few-shot learning的方法途径。这里提一嘴meta-dataset存在的一个bias，即测试时shot和way普遍偏高，这导致partial fine-tune方法重现江湖，但实验后发现这些方法在1-shot和5-shot表现不佳，是值得注意的点。

